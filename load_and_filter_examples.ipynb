{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/v1.0/dev/nq-dev-0*.jsonl.gz\"\n",
    "#DATA_DIR = \"data/tiny-dev/nq-dev-sample.jsonl.gz\"\n",
    "max_position = 50\n",
    "skip_nested_contexts = False\n",
    "TextSpan = collections.namedtuple(\"TextSpan\", \"token_positions text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/v1.0/dev/nq-dev-0*.jsonl.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-578d405463a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__builtin__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Issue #13781: os.fdopen() creates a fileobj with a bogus name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/v1.0/dev/nq-dev-0*.jsonl.gz'"
     ]
    }
   ],
   "source": [
    "\n",
    "examples = []\n",
    "with gzip.GzipFile(DATA_DIR) as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line,'utf-8')\n",
    "        #print example['example_id']\n",
    "        a = example['annotations']\n",
    "        #long_answers = [ a['long_answer'] for a in example['annotations'] if a['long_answer']['start_byte'] >= 0]\n",
    "        print a\n",
    "        '''\n",
    "        long_answer_bounds = [\n",
    "          (la['start_byte'], la['end_byte']) for la in long_answers\n",
    "          ]\n",
    "        #print long_answer_bounds\n",
    "        long_answer_counts = [\n",
    "          long_answer_bounds.count(la) for la in long_answer_bounds\n",
    "          ]\n",
    "        '''\n",
    "        print(example[\"long_answer_candidates\"])\n",
    "        #print long_answer_counts\n",
    "        #print example.keys()\n",
    "        #print \"short_answers\", a[\"short_answers\"]\n",
    "        #if len(a[\"short_answers\"]) > 1 and len(a[\"long_answer\"]) > 1:\n",
    "        #    print (a)\n",
    "        ''' if len(a[\"short_answers\"]) > 1:\n",
    "            print long_answers\n",
    "            print long_answer_counts\n",
    "            print \"short_answers\", a[\"short_answers\"]\n",
    "        '''\n",
    "        examples.append(example)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES_COUNT = {\"Paragraph\": 0, \"Table\" : 0, \"List\" : 0, \"Other\" : 0}\n",
    "def get_text_span(example, span):\n",
    "    \"\"\"Returns the text in the example's document in the given token span.\"\"\"\n",
    "    token_positions = []\n",
    "    tokens = []\n",
    "    for i in range(span[\"start_token\"], span[\"end_token\"]):\n",
    "        t = example[\"document_tokens\"][i]\n",
    "        if not t[\"html_token\"]:\n",
    "            token_positions.append(i)\n",
    "            token = t[\"token\"].replace(\" \", \"\")\n",
    "            tokens.append(token)\n",
    "    return TextSpan(token_positions, \" \".join(tokens))\n",
    "\n",
    "def get_candidate_text(e, idx):\n",
    "    \"\"\"Returns a text representation of the candidate at the given index.\"\"\"\n",
    "    # No candidate at this index.\n",
    "    if idx < 0 or idx >= len(e[\"long_answer_candidates\"]):\n",
    "        return TextSpan([], \"\")\n",
    "    # This returns an actual candidate.\n",
    "    return get_text_span(e, e[\"long_answer_candidates\"][idx])\n",
    "\n",
    "def should_skip_context(e, idx):\n",
    "    t = get_candidate_type(e, idx)\n",
    "    CANDIDATES_COUNT[t] += 1\n",
    "    if (skip_nested_contexts and\n",
    "        not e[\"long_answer_candidates\"][idx][\"top_level\"]):\n",
    "        return True\n",
    "    elif not get_candidate_text(e, idx).text.strip():\n",
    "        # Skip empty contexts.\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def candidates_iter(e):\n",
    "    \"\"\"Yield's the candidates that should not be skipped in an example.\"\"\"\n",
    "    for idx, c in enumerate(e[\"long_answer_candidates\"]):\n",
    "        if should_skip_context(e, idx):\n",
    "            continue\n",
    "    yield idx, c\n",
    "\n",
    "def get_candidate_type(e, idx):\n",
    "    \"\"\"Returns the candidate's type: Table, Paragraph, List or Other.\"\"\"\n",
    "    c = e[\"long_answer_candidates\"][idx]\n",
    "    first_token = e[\"document_tokens\"][c[\"start_token\"]][\"token\"]\n",
    "    if first_token == \"<Table>\":\n",
    "        return \"Table\"\n",
    "    elif first_token == \"<P>\":\n",
    "        return \"Paragraph\"\n",
    "    elif first_token in (\"<Ul>\", \"<Dl>\", \"<Ol>\"):\n",
    "        return \"List\"\n",
    "    elif first_token in (\"<Tr>\", \"<Li>\", \"<Dd>\", \"<Dt>\"):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        tf.logging.warning(\"Unknoww candidate type found: %s\", first_token)\n",
    "    return \"Other\"\n",
    "\n",
    "def add_candidate_types_and_positions(e):\n",
    "    \"\"\"Adds type and position info to each candidate in the document.\"\"\"\n",
    "    counts = collections.defaultdict(int)\n",
    "    for idx, c in candidates_iter(e):\n",
    "        context_type = get_candidate_type(e, idx)\n",
    "        if counts[context_type] < max_position:\n",
    "            counts[context_type] += 1\n",
    "            c[\"type_and_position\"] = \"[%s=%d]\" % (context_type, counts[context_type])\n",
    "\n",
    "def load_example(line):\n",
    "    e = json.loads(line, object_pairs_hook=collections.OrderedDict)\n",
    "    add_candidate_types_and_positions(e)\n",
    "    \n",
    "def read_nq_examples(input_file, is_training):\n",
    "    \"\"\"Read a NQ json file into a list of NqExample.\"\"\"\n",
    "    input_paths = tf.gfile.Glob(input_file)\n",
    "    input_data = []\n",
    "\n",
    "    def _open(path):\n",
    "        if path.endswith(\".gz\"):\n",
    "            return gzip.GzipFile(fileobj=tf.gfile.Open(path, \"r\"))\n",
    "        else:\n",
    "            return tf.gfile.Open(path, \"r\")\n",
    "\n",
    "    for path in input_paths:\n",
    "        tf.logging.info(\"Reading: %s\", path)\n",
    "        with _open(path) as input_file:\n",
    "            for line in input_file:\n",
    "                load_example(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0916 15:49:41.950619 140638234494784 <ipython-input-40-91a87fe9d373>:82] Reading: data/v1.0/dev/nq-dev-04.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "read_nq_examples(DATA_DIR, False)\n",
    "tf.logging.info(\"Disrtribution of long answer types: %s\",CANDIDATES_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
